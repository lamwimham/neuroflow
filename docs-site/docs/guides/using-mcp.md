# ä½¿ç”¨ MCP æœåŠ¡

MCP (Model Context Protocol) æä¾›ä¸ AI æ¨¡å‹æœåŠ¡çš„æ ‡å‡†åŒ–é›†æˆï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€åµŒå…¥å‘é‡ç­‰åŠŸèƒ½ã€‚

## ä»€ä¹ˆæ˜¯ MCP?

MCP æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„åè®®ï¼Œç”¨äº:

- ğŸ¤– **æ–‡æœ¬ç”Ÿæˆ**: è°ƒç”¨ LLM ç”Ÿæˆæ–‡æœ¬
- ğŸ”¢ **åµŒå…¥å‘é‡**: è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
- ğŸ¨ **å¤šæ¨¡æ€å¤„ç†**: å›¾åƒã€éŸ³é¢‘ç­‰
- ğŸ”„ **æµå¼å“åº”**: å®æ—¶ç”Ÿæˆå†…å®¹

## MCP å®¢æˆ·ç«¯

### åŸºç¡€ä½¿ç”¨

```python
from neuroflow import agent, BaseAgent

@agent(name="mcp_agent")
class MCPAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        prompt = request.get("prompt")
        
        # ä½¿ç”¨ MCP ç”Ÿæˆæ–‡æœ¬
        response = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={
                "temperature": 0.7,
                "max_tokens": 100
            }
        )
        
        return {"response": response}
```

### è·å–åµŒå…¥å‘é‡

```python
@agent(name="embedding_agent")
class EmbeddingAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        texts = request.get("texts", [])
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = await self.get_embeddings(
            texts=texts,
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        return {
            "embeddings": embeddings,
            "dimensions": len(embeddings[0]) if embeddings else 0
        }
```

## å®ç”¨ç¤ºä¾‹

### 1. æ–‡æœ¬æ‘˜è¦ Agent

```python
from neuroflow import agent, BaseAgent

@agent(name="summarizer", description="æ–‡æœ¬æ‘˜è¦ Agent")
class SummarizerAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        text = request.get("text")
        max_length = request.get("max_length", 100)
        
        prompt = f"""
Summarize the following text in Chinese, keeping it under {max_length} characters:

{text}

Summary:
"""
        
        summary = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={
                "temperature": 0.3,
                "max_tokens": 200
            }
        )
        
        return {"summary": summary}
```

### 2. æ–‡æœ¬åˆ†ç±» Agent

```python
from neuroflow import agent, BaseAgent

@agent(name="classifier", description="æ–‡æœ¬åˆ†ç±» Agent")
class ClassifierAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        text = request.get("text")
        categories = request.get("categories", ["positive", "negative", "neutral"])
        
        prompt = f"""
Classify the following text into one of these categories: {', '.join(categories)}

Text: {text}

Category:
"""
        
        category = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={
                "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§
                "max_tokens": 20
            }
        )
        
        return {"category": category.strip()}
```

### 3. è¯­ä¹‰æœç´¢ Agent

```python
import numpy as np

@agent(name="semantic_search", description="è¯­ä¹‰æœç´¢ Agent")
class SemanticSearchAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        query = request.get("query")
        documents = request.get("documents", [])
        top_k = request.get("top_k", 3)
        
        # è·å–æŸ¥è¯¢å’Œæ–‡æ¡£çš„åµŒå…¥
        all_texts = [query] + documents
        embeddings = await self.get_embeddings(
            texts=all_texts,
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        query_embedding = np.array(embeddings[0])
        doc_embeddings = np.array(embeddings[1:])
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for i, doc_emb in enumerate(doc_embeddings):
            sim = np.dot(query_embedding, doc_emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)
            )
            similarities.append((i, float(sim)))
        
        # æ’åºå¹¶è¿”å› top_k
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = [
            {"index": idx, "score": score, "document": documents[idx]}
            for idx, score in similarities[:top_k]
        ]
        
        return {"results": results}
```

### 4. å¯¹è¯æœºå™¨äºº Agent

```python
@agent(name="chatbot", description="å¯¹è¯æœºå™¨äºº Agent")
class ChatbotAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        user_id = request.get("user_id")
        message = request.get("message")
        
        # è·å–å¯¹è¯å†å²
        history = self._get_history(user_id)
        
        # æ„å»ºæç¤º
        prompt = self._build_prompt(history, message)
        
        # ç”Ÿæˆå›å¤
        response = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={
                "temperature": 0.7,
                "max_tokens": 150
            }
        )
        
        # æ›´æ–°å†å²
        self._update_history(user_id, message, response)
        
        return {
            "response": response,
            "conversation_id": user_id
        }
    
    def _get_history(self, user_id: str) -> list:
        """è·å–å¯¹è¯å†å²"""
        key = f"chat_history_{user_id}"
        return self.retrieve_memory(key) or []
    
    def _update_history(self, user_id: str, user_msg: str, bot_resp: str):
        """æ›´æ–°å¯¹è¯å†å²"""
        key = f"chat_history_{user_id}"
        history = self._get_history(user_id)
        history.append({"user": user_msg, "bot": bot_resp})
        
        # åªä¿ç•™æœ€è¿‘ 10 è½®
        if len(history) > 10:
            history = history[-10:]
        
        self.store_memory(key, history, "long_term")
    
    def _build_prompt(self, history: list, new_message: str) -> str:
        """æ„å»ºæç¤º"""
        prompt = "You are a helpful assistant.\n\n"
        
        for turn in history[-5:]:  # æœ€è¿‘ 5 è½®
            prompt += f"User: {turn['user']}\n"
            prompt += f"Assistant: {turn['bot']}\n"
        
        prompt += f"User: {new_message}\n"
        prompt += "Assistant: "
        
        return prompt
```

### 5. ä»£ç ç”Ÿæˆ Agent

```python
@agent(name="code_generator", description="ä»£ç ç”Ÿæˆ Agent")
class CodeGeneratorAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        description = request.get("description")
        language = request.get("language", "python")
        
        prompt = f"""
Generate {language} code based on the following description:

{description}

Requirements:
- Write clean, readable code
- Include comments
- Follow best practices

Code:
"""
        
        code = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={
                "temperature": 0.2,  # ä½æ¸©åº¦ç¡®ä¿ä»£ç å‡†ç¡®æ€§
                "max_tokens": 500
            }
        )
        
        return {
            "code": code,
            "language": language
        }
```

### 6. ç¿»è¯‘ Agent

```python
@agent(name="translator", description="ç¿»è¯‘ Agent")
class TranslatorAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        text = request.get("text")
        source_lang = request.get("source_lang", "auto")
        target_lang = request.get("target_lang", "en")
        
        if source_lang == "auto":
            prompt = f"""
Translate the following text to {target_lang}:

{text}

Translation:
"""
        else:
            prompt = f"""
Translate the following text from {source_lang} to {target_lang}:

{text}

Translation:
"""
        
        translation = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={
                "temperature": 0.3,
                "max_tokens": 300
            }
        )
        
        return {
            "translation": translation,
            "source_lang": source_lang,
            "target_lang": target_lang
        }
```

## é«˜çº§ç”¨æ³•

### æµå¼ç”Ÿæˆ

```python
@agent(name="streaming_agent")
class StreamingAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        prompt = request.get("prompt")
        
        # æµå¼ç”Ÿæˆ (ä¼ªä»£ç ï¼Œå®é™…å®ç°å–å†³äº MCP æœåŠ¡ç«¯)
        async for chunk in self.generate_text_stream(
            prompt=prompt,
            model="gpt-3.5-turbo"
        ):
            yield {"chunk": chunk}
```

### å¤šæ¨¡å‹åä½œ

```python
@agent(name="multi_model_agent")
class MultiModelAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        prompt = request.get("prompt")
        
        # ä½¿ç”¨ä¸åŒæ¨¡å‹å¤„ç†ä¸åŒä»»åŠ¡
        # 1. ä½¿ç”¨å¿«é€Ÿæ¨¡å‹ç”Ÿæˆè‰ç¨¿
        draft = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo",
            params={"temperature": 0.7}
        )
        
        # 2. ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹ä¼˜åŒ–
        refined = await self.generate_text(
            prompt=f"Improve the following text:\n\n{draft}",
            model="gpt-4",
            params={"temperature": 0.3}
        )
        
        return {
            "draft": draft,
            "refined": refined
        }
```

### æç¤ºå·¥ç¨‹

```python
@agent(name="prompt_engineer")
class PromptEngineerAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        task = request.get("task")
        context = request.get("context", "")
        examples = request.get("examples", [])
        
        # æ„å»ºç»“æ„åŒ–æç¤º
        prompt = self._build_structured_prompt(task, context, examples)
        
        response = await self.generate_text(
            prompt=prompt,
            model="gpt-3.5-turbo"
        )
        
        return {"response": response}
    
    def _build_structured_prompt(
        self,
        task: str,
        context: str,
        examples: list
    ) -> str:
        """æ„å»ºç»“æ„åŒ–æç¤º"""
        prompt = "You are an expert assistant.\n\n"
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context:
            prompt += f"Context:\n{context}\n\n"
        
        # æ·»åŠ ç¤ºä¾‹ (Few-shot)
        if examples:
            prompt += "Examples:\n"
            for example in examples:
                prompt += f"Input: {example['input']}\n"
                prompt += f"Output: {example['output']}\n\n"
        
        # æ·»åŠ ä»»åŠ¡
        prompt += f"Task:\n{task}\n\n"
        prompt += "Response:\n"
        
        return prompt
```

## é…ç½® MCP å®¢æˆ·ç«¯

### è‡ªå®šä¹‰ç«¯ç‚¹

```python
from neuroflow.agent import MCPClient

# ä½¿ç”¨è‡ªå®šä¹‰ MCP ç«¯ç‚¹
mcp_client = MCPClient(endpoint="http://your-mcp-server.com/mcp")

async with mcp_client as client:
    embeddings = await client.get_embeddings(["text1", "text2"])
    text = await client.generate_text(prompt="Hello")
```

### æ¨¡å‹é€‰æ‹©

```python
# ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒæ¨¡å‹

# åµŒå…¥å‘é‡
embeddings = await self.get_embeddings(
    texts=["text1", "text2"],
    model="sentence-transformers/all-MiniLM-L6-v2"  # è½»é‡çº§
)

# å¿«é€Ÿå“åº”
fast_response = await self.generate_text(
    prompt="Quick answer",
    model="gpt-3.5-turbo"  # å¿«é€Ÿ
)

# é«˜è´¨é‡å“åº”
quality_response = await self.generate_text(
    prompt="Detailed analysis",
    model="gpt-4"  # é«˜è´¨é‡
)
```

## æœ€ä½³å®è·µ

### 1. æç¤ºä¼˜åŒ–

```python
# âŒ æ¨¡ç³Šçš„æç¤º
prompt = "Tell me about AI"

# âœ… å…·ä½“çš„æç¤º
prompt = """
Provide a concise introduction to artificial intelligence (AI) covering:
1. Definition
2. Key applications
3. Current trends

Keep it under 200 words.
"""
```

### 2. å‚æ•°è°ƒä¼˜

```python
# åˆ›é€ æ€§ä»»åŠ¡ (é«˜ temperature)
creative = await self.generate_text(
    prompt="Write a poem",
    model="gpt-3.5-turbo",
    params={"temperature": 0.8, "top_p": 0.9}
)

# äº‹å®æ€§ä»»åŠ¡ (ä½ temperature)
factual = await self.generate_text(
    prompt="Explain quantum computing",
    model="gpt-3.5-turbo",
    params={"temperature": 0.2, "top_p": 0.5}
)
```

### 3. é”™è¯¯å¤„ç†

```python
@agent(name="robust_mcp_agent")
class RobustMCPAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        prompt = request.get("prompt")
        
        try:
            response = await self.generate_text(
                prompt=prompt,
                model="gpt-3.5-turbo",
                params={"timeout": 30}
            )
            return {"response": response}
        
        except TimeoutError:
            self.context.logger.error("MCP request timed out")
            return {"error": "Request timeout"}
        
        except Exception as e:
            self.context.logger.exception(f"MCP error: {e}")
            return {"error": str(e)}
```

### 4. æˆæœ¬æ§åˆ¶

```python
@agent(name="cost_aware_agent")
class CostAwareAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        prompt = request.get("prompt")
        budget = request.get("budget", "low")
        
        # æ ¹æ®é¢„ç®—é€‰æ‹©æ¨¡å‹
        if budget == "low":
            model = "gpt-3.5-turbo"
            max_tokens = 100
        elif budget == "medium":
            model = "gpt-3.5-turbo"
            max_tokens = 500
        else:
            model = "gpt-4"
            max_tokens = 1000
        
        response = await self.generate_text(
            prompt=prompt,
            model=model,
            params={"max_tokens": max_tokens}
        )
        
        return {
            "response": response,
            "model": model,
            "tokens_used": max_tokens
        }
```

## è°ƒè¯•å’Œç›‘æ§

### æ—¥å¿—è®°å½•

```python
@agent(name="logged_mcp_agent")
class LoggedMCPAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        prompt = request.get("prompt")
        
        self.context.logger.info(f"MCP request: {prompt[:100]}...")
        
        response = await self.generate_text(prompt=prompt)
        
        self.context.logger.info(f"MCP response: {response[:100]}...")
        
        return {"response": response}
```

### æ€§èƒ½ç›‘æ§

```python
import time

@agent(name="monitored_agent")
class MonitoredAgent(BaseAgent):
    async def handle(self, request: dict) -> dict:
        start = time.time()
        
        response = await self.generate_text(
            prompt=request.get("prompt")
        )
        
        elapsed = time.time() - start
        
        return {
            "response": response,
            "latency_ms": elapsed * 1000
        }
```

## ä¸‹ä¸€æ­¥

- ğŸ¤– **[æ„å»º Agent](building-agents.md)** - ä½¿ç”¨ MCP åˆ›å»º Agent
- ğŸ› ï¸ **[å¼€å‘å·¥å…·](developing-tools.md)** - é›†æˆ MCP å·¥å…·
- ğŸ“Š **[æ€§èƒ½ä¼˜åŒ–](../best-practices/performance.md)** - ä¼˜åŒ– MCP è°ƒç”¨
- ğŸ”’ **[å®‰å…¨å®è·µ](../best-practices/security.md)** - MCP å®‰å…¨è€ƒè™‘

---

**å‚è€ƒèµ„æº**:
- [MCP è§„èŒƒ](https://modelcontextprotocol.io/)
- [ç¤ºä¾‹ä»£ç ](../examples/advanced.md)
- [æ•…éšœæ’é™¤](../troubleshooting/faq.md)
