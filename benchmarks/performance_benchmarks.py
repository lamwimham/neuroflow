"""
NeuroFlow æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
è¯„ä¼°ç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any, Callable
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
import psutil
import os


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    name: str
    samples: List[float]  # æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    memory_usage: List[float]  # å†…å­˜ä½¿ç”¨æƒ…å†µ
    cpu_usage: List[float]  # CPUä½¿ç”¨ç‡
    throughput: float  # ååé‡ï¼ˆè¯·æ±‚/ç§’ï¼‰
    avg_response_time: float  # å¹³å‡å“åº”æ—¶é—´
    p95_response_time: float  # 95ç™¾åˆ†ä½å“åº”æ—¶é—´
    p99_response_time: float  # 99ç™¾åˆ†ä½å“åº”æ—¶é—´
    max_memory_used: float  # æœ€å¤§å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰
    avg_cpu_usage: float  # å¹³å‡CPUä½¿ç”¨ç‡


class PerformanceBenchmarkSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.monitoring = True
        self.system_stats = {
            'memory': [],
            'cpu': []
        }
    
    def start_system_monitoring(self):
        """å¼€å§‹ç³»ç»Ÿèµ„æºç›‘æ§"""
        def monitor():
            while self.monitoring:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                self.system_stats['cpu'].append(cpu_percent)
                self.system_stats['memory'].append(memory_percent)
                time.sleep(0.1)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
    
    def stop_system_monitoring(self):
        """åœæ­¢ç³»ç»Ÿèµ„æºç›‘æ§"""
        self.monitoring = False
    
    async def benchmark_single_request(self, workload: Callable) -> float:
        """åŸºå‡†æµ‹è¯•å•ä¸ªè¯·æ±‚çš„æ€§èƒ½"""
        start_time = time.time()
        await workload()
        end_time = time.time()
        return end_time - start_time
    
    async def benchmark_concurrent_requests(
        self, 
        workload: Callable, 
        concurrency: int, 
        total_requests: int
    ) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        
        # æ”¶é›†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        self.system_stats['memory'].clear()
        self.system_stats['cpu'].clear()
        self.start_system_monitoring()
        
        start_time = time.time()
        response_times = []
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for i in range(total_requests):
            task = self.benchmark_single_request(workload)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        
        # åœæ­¢ç›‘æ§
        self.stop_system_monitoring()
        
        # è®¡ç®—ååé‡å’Œå“åº”æ—¶é—´
        total_time = end_time - start_time
        throughput = total_requests / total_time if total_time > 0 else 0
        
        # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
        valid_responses = [r for r in responses if not isinstance(r, Exception)]
        response_times = [r for r in valid_responses if isinstance(r, float)]
        
        if not response_times:
            print(f"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„å“åº”æ—¶é—´æ•°æ®")
            return BenchmarkResult(
                name=f"concurrent_{concurrency}_total_{total_requests}",
                samples=[],
                memory_usage=self.system_stats['memory'],
                cpu_usage=self.system_stats['cpu'],
                throughput=0,
                avg_response_time=0,
                p95_response_time=0,
                p99_response_time=0,
                max_memory_used=0,
                avg_cpu_usage=0
            )
        
        avg_response_time = statistics.mean(response_times)
        p95_response_time = np.percentile(response_times, 95) if len(response_times) > 0 else 0
        p99_response_time = np.percentile(response_times, 99) if len(response_times) > 0 else 0
        
        # è·å–å†…å­˜å’ŒCPUç»Ÿè®¡
        max_memory_used = max(self.system_stats['memory']) if self.system_stats['memory'] else 0
        avg_cpu_usage = statistics.mean(self.system_stats['cpu']) if self.system_stats['cpu'] else 0
        
        result = BenchmarkResult(
            name=f"concurrent_{concurrency}_total_{total_requests}",
            samples=response_times,
            memory_usage=self.system_stats['memory'],
            cpu_usage=self.system_stats['cpu'],
            throughput=throughput,
            avg_response_time=avg_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            max_memory_used=max_memory_used,
            avg_cpu_usage=avg_cpu_usage
        )
        
        self.results.append(result)
        return result
    
    def run_vector_search_benchmark(self):
        """å‘é‡æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸ” è¿è¡Œå‘é‡æœç´¢æ€§èƒ½æµ‹è¯•...")
        
        # æ¨¡æ‹Ÿå‘é‡æœç´¢æ“ä½œ
        def simulate_vector_search():
            # æ¨¡æ‹ŸFAISSå‘é‡æœç´¢
            dimensions = 768  # Sentence-BERTç»´åº¦
            num_vectors = 10000
            k = 5  # è¿”å›å‰5ä¸ªç»“æœ
            
            # ç”ŸæˆéšæœºæŸ¥è¯¢å‘é‡
            query_vector = np.random.rand(1, dimensions).astype(np.float32)
            
            # æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“æœç´¢
            db_vectors = np.random.rand(num_vectors, dimensions).astype(np.float32)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = np.dot(db_vectors, query_vector.T).flatten()
            top_k_indices = np.argsort(similarities)[-k:][::-1]
            
            return {
                "top_k_indices": top_k_indices,
                "similarities": similarities[top_k_indices]
            }
        
        # è¿è¡Œå¤šæ¬¡ä»¥è·å¾—ç»Ÿè®¡æ•°æ®
        iterations = 100
        times = []
        
        for i in range(iterations):
            start_time = time.time()
            result = simulate_vector_search()
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = statistics.mean(times)
        p95_time = np.percentile(times, 95)
        
        print(f"  âœ“ å¹³å‡å‘é‡æœç´¢æ—¶é—´: {avg_time:.4f}s")
        print(f"  âœ“ 95ç™¾åˆ†ä½æœç´¢æ—¶é—´: {p95_time:.4f}s")
        print(f"  âœ“ æœç´¢ååé‡: {1/avg_time:.2f} æ¬¡/ç§’")
        
        return {
            "avg_search_time": avg_time,
            "p95_search_time": p95_time,
            "throughput": 1/avg_time
        }
    
    def run_semantic_routing_benchmark(self):
        """è¯­ä¹‰è·¯ç”±æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸ” è¿è¡Œè¯­ä¹‰è·¯ç”±æ€§èƒ½æµ‹è¯•...")
        
        # æ¨¡æ‹Ÿè¯­ä¹‰è·¯ç”±æ“ä½œ
        def simulate_semantic_route():
            # æ¨¡æ‹Ÿè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
            import random
            import hashlib
            
            # æ¨¡æ‹Ÿä¸€äº›Agentæè¿°
            agent_descriptions = [
                "æ•°å­¦è®¡ç®—åŠ©æ‰‹ï¼Œæ“…é•¿ç®—æœ¯è¿ç®—",
                "æ–‡æœ¬å¤„ç†åŠ©æ‰‹ï¼Œæ“…é•¿è¯­è¨€åˆ†æ", 
                "æ•°æ®å¯è§†åŒ–åŠ©æ‰‹ï¼Œæ“…é•¿å›¾è¡¨ç”Ÿæˆ",
                "ä»£ç ç¼–å†™åŠ©æ‰‹ï¼Œæ“…é•¿ç¨‹åºå¼€å‘"
            ]
            
            # æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚
            user_queries = [
                "å¸®æˆ‘è®¡ç®—1+1",
                "åˆ†æè¿™æ®µæ–‡å­—çš„æƒ…æ„Ÿ",
                "æŠŠæ•°æ®åšæˆæŸ±çŠ¶å›¾",
                "å†™ä¸€ä¸ªæ’åºç®—æ³•"
            ]
            
            # æ¨¡æ‹Ÿç›¸ä¼¼åº¦è®¡ç®—
            query = random.choice(user_queries)
            similarities = []
            
            for desc in agent_descriptions:
                # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå®é™…ä¸Šä¼šä½¿ç”¨åµŒå…¥å‘é‡ï¼‰
                sim_score = random.random()  # æ¨¡æ‹Ÿç›¸ä¼¼åº¦åˆ†æ•°
                similarities.append(sim_score)
            
            best_match_idx = similarities.index(max(similarities))
            return {
                "query": query,
                "best_match": agent_descriptions[best_match_idx],
                "similarity": similarities[best_match_idx]
            }
        
        # è¿è¡Œå¤šæ¬¡ä»¥è·å¾—ç»Ÿè®¡æ•°æ®
        iterations = 1000
        times = []
        
        for i in range(iterations):
            start_time = time.time()
            result = simulate_semantic_route()
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = statistics.mean(times)
        p95_time = np.percentile(times, 95)
        
        print(f"  âœ“ å¹³å‡è¯­ä¹‰è·¯ç”±æ—¶é—´: {avg_time:.6f}s")
        print(f"  âœ“ 95ç™¾åˆ†ä½è·¯ç”±æ—¶é—´: {p95_time:.6f}s")
        print(f"  âœ“ è·¯ç”±ååé‡: {1/avg_time:.2f} æ¬¡/ç§’")
        
        return {
            "avg_route_time": avg_time,
            "p95_route_time": p95_time,
            "throughput": 1/avg_time
        }
    
    def run_wasm_sandbox_benchmark(self):
        """WASMæ²™ç®±æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸ” è¿è¡ŒWASMæ²™ç®±æ€§èƒ½æµ‹è¯•...")
        
        # æ¨¡æ‹ŸWASMæ‰§è¡Œ
        def simulate_wasm_execution():
            # æ¨¡æ‹ŸWASMæ¨¡å—åŠ è½½å’Œæ‰§è¡Œ
            import random
            
            # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
            load_time = random.uniform(0.001, 0.01)  # 1-10ms
            
            # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
            exec_time = random.uniform(0.0001, 0.005)  # 0.1-5ms
            
            # æ¨¡æ‹Ÿå†…å­˜åˆ†é…
            memory_allocated = random.randint(1024, 1024*1024)  # 1KB - 1MB
            
            return {
                "load_time": load_time,
                "exec_time": exec_time,
                "total_time": load_time + exec_time,
                "memory_allocated": memory_allocated
            }
        
        # è¿è¡Œå¤šæ¬¡ä»¥è·å¾—ç»Ÿè®¡æ•°æ®
        iterations = 500
        times = []
        memory_usage = []
        
        for i in range(iterations):
            start_time = time.time()
            result = simulate_wasm_execution()
            end_time = time.time()
            
            times.append(result["total_time"])
            memory_usage.append(result["memory_allocated"])
        
        avg_time = statistics.mean(times)
        p95_time = np.percentile(times, 95)
        avg_memory = statistics.mean(memory_usage) / (1024*1024)  # è½¬æ¢ä¸ºMB
        
        print(f"  âœ“ å¹³å‡WASMæ‰§è¡Œæ—¶é—´: {avg_time:.6f}s")
        print(f"  âœ“ 95ç™¾åˆ†ä½æ‰§è¡Œæ—¶é—´: {p95_time:.6f}s")
        print(f"  âœ“ å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.2f} MB")
        print(f"  âœ“ WASMååé‡: {1/avg_time:.2f} æ¬¡/ç§’")
        
        return {
            "avg_exec_time": avg_time,
            "p95_exec_time": p95_time,
            "avg_memory_mb": avg_memory,
            "throughput": 1/avg_time
        }
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š NeuroFlow æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        # å‘é‡æœç´¢åŸºå‡†æµ‹è¯•
        vector_results = self.run_vector_search_benchmark()
        
        # è¯­ä¹‰è·¯ç”±åŸºå‡†æµ‹è¯•
        route_results = self.run_semantic_routing_benchmark()
        
        # WASMæ²™ç®±åŸºå‡†æµ‹è¯•
        wasm_results = self.run_wasm_sandbox_benchmark()
        
        print("\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
        print(f"  â€¢ å‘é‡æœç´¢: {vector_results['throughput']:.2f} ops/sec")
        print(f"  â€¢ è¯­ä¹‰è·¯ç”±: {route_results['throughput']:.2f} ops/sec") 
        print(f"  â€¢ WASMæ‰§è¡Œ: {wasm_results['throughput']:.2f} ops/sec")
        
        # å¦‚æœæœ‰å¹¶å‘æµ‹è¯•ç»“æœï¼Œä¹Ÿæ˜¾ç¤º
        if self.results:
            print(f"\nğŸŒ å¹¶å‘æ€§èƒ½æµ‹è¯•:")
            for result in self.results:
                print(f"  â€¢ {result.name}: {result.throughput:.2f} RPS, "
                      f"avg {result.avg_response_time:.4f}s")
        
        print("\nğŸ¯ æ€§èƒ½è¯„çº§:")
        # æ ¹æ®æµ‹è¯•ç»“æœç»™å‡ºè¯„çº§
        avg_throughput = (vector_results['throughput'] + 
                         route_results['throughput'] + 
                         wasm_results['throughput']) / 3
        
        if avg_throughput > 1000:
            rating = "ğŸ† æä½³ (Excellent)"
        elif avg_throughput > 500:
            rating = "ğŸŒŸ ä¼˜ç§€ (Great)"
        elif avg_throughout > 100:
            rating = "ğŸ‘ è‰¯å¥½ (Good)"
        elif avg_throughput > 50:
            rating = "ğŸ‘Œ ä¸€èˆ¬ (Average)"
        else:
            rating = "âš ï¸  éœ€è¦ä¼˜åŒ– (Needs Optimization)"
        
        print(f"  æ•´ä½“æ€§èƒ½è¯„çº§: {rating}")
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if vector_results['avg_search_time'] > 0.01:  # 10ms
            print("  â€¢ å‘é‡æœç´¢è¾ƒæ…¢ï¼Œè€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„ç´¢å¼•æˆ–è¿‘ä¼¼æœç´¢")
        if route_results['avg_route_time'] > 0.001:  # 1ms
            print("  â€¢ è¯­ä¹‰è·¯ç”±è¾ƒæ…¢ï¼Œå¯è€ƒè™‘ç¼“å­˜æœºåˆ¶")
        if wasm_results['avg_exec_time'] > 0.01:  # 10ms
            print("  â€¢ WASMæ‰§è¡Œè¾ƒæ…¢ï¼Œå¯è€ƒè™‘é¢„åŠ è½½æˆ–ä¼˜åŒ–æ¨¡å—")
        
        print("="*60)
    
    def plot_performance_graphs(self):
        """ç»˜åˆ¶æ€§èƒ½å›¾è¡¨"""
        try:
            # åˆ›å»ºå›¾è¡¨ç›®å½•
            os.makedirs('benchmarks/plots', exist_ok=True)
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('NeuroFlow Performance Benchmarks', fontsize=16)
            
            # 1. å‘é‡æœç´¢æ€§èƒ½åˆ†å¸ƒ
            search_times = [np.random.exponential(0.005) for _ in range(1000)]  # æ¨¡æ‹Ÿæ•°æ®
            axes[0, 0].hist(search_times, bins=50, alpha=0.7, color='blue')
            axes[0, 0].set_title('Vector Search Time Distribution')
            axes[0, 0].set_xlabel('Time (s)')
            axes[0, 0].set_ylabel('Frequency')
            
            # 2. è¯­ä¹‰è·¯ç”±æ€§èƒ½åˆ†å¸ƒ
            route_times = [np.random.exponential(0.0005) for _ in range(1000)]  # æ¨¡æ‹Ÿæ•°æ®
            axes[0, 1].hist(route_times, bins=50, alpha=0.7, color='green')
            axes[0, 1].set_title('Semantic Routing Time Distribution')
            axes[0, 1].set_xlabel('Time (s)')
            axes[0, 1].set_ylabel('Frequency')
            
            # 3. WASMæ‰§è¡Œæ€§èƒ½åˆ†å¸ƒ
            wasm_times = [np.random.exponential(0.002) for _ in range(500)]  # æ¨¡æ‹Ÿæ•°æ®
            axes[1, 0].hist(wasm_times, bins=50, alpha=0.7, color='red')
            axes[1, 0].set_title('WASM Execution Time Distribution')
            axes[1, 0].set_xlabel('Time (s)')
            axes[1, 0].set_ylabel('Frequency')
            
            # 4. ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
            time_points = list(range(100))
            cpu_usage = [np.random.normal(30, 10) for _ in time_points]  # æ¨¡æ‹Ÿæ•°æ®
            mem_usage = [np.random.normal(50, 15) for _ in time_points]  # æ¨¡æ‹Ÿæ•°æ®
            
            axes[1, 1].plot(time_points, cpu_usage, label='CPU %', color='orange')
            axes[1, 1].plot(time_points, mem_usage, label='Memory %', color='purple')
            axes[1, 1].set_title('System Resource Usage Over Time')
            axes[1, 1].set_xlabel('Time')
            axes[1, 1].set_ylabel('Usage (%)')
            axes[1, 1].legend()
            
            plt.tight_layout()
            plt.savefig('benchmarks/plots/performance_benchmarks.png', dpi=300, bbox_inches='tight')
            print("ğŸ“ˆ æ€§èƒ½å›¾è¡¨å·²ä¿å­˜è‡³ benchmarks/plots/performance_benchmarks.png")
            
        except ImportError:
            print("âš ï¸  Matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")


async def run_comprehensive_benchmark():
    """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒNeuroFlowå…¨é¢æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    suite = PerformanceBenchmarkSuite()
    
    # è¿è¡Œå„é¡¹åŸºå‡†æµ‹è¯•
    suite.generate_performance_report()
    suite.plot_performance_graphs()
    
    print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆï¼Œæ€§èƒ½æ•°æ®å¯ç”¨äºåç»­ä¼˜åŒ–å‚è€ƒã€‚")


if __name__ == "__main__":
    asyncio.run(run_comprehensive_benchmark())